{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%set_env TORCH=1\n",
    "%set_env KOPT=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import trange\n",
    "from tinygrad.tensor import Tensor\n",
    "from tinygrad.nn import optim\n",
    "from datasets import fetch_mnist\n",
    "np.random.seed(1337)\n",
    "Tensor.manual_seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator:\n",
    "    def __init__(self, input_dim: int, output_dim: int):\n",
    "        self.l1 = Tensor.scaled_uniform(input_dim, 256)\n",
    "        self.l2 = Tensor.scaled_uniform(256, 512)\n",
    "        self.l3 = Tensor.scaled_uniform(512, 1024)\n",
    "        self.l4 = Tensor.scaled_uniform(1024, output_dim)\n",
    "    \n",
    "    def __call__(self, x: Tensor) -> Tensor:\n",
    "        x = x.dot(self.l1).leakyrelu(0.2)\n",
    "        x = x.dot(self.l2).leakyrelu(0.2)\n",
    "        x = x.dot(self.l3).leakyrelu(0.2)\n",
    "        x = x.dot(self.l4).tanh()\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator:\n",
    "    def __init__(self, input_dim: int):\n",
    "        self.l1 = Tensor.scaled_uniform(input_dim, 1024)\n",
    "        self.l2 = Tensor.scaled_uniform(1024, 512)\n",
    "        self.l3 = Tensor.scaled_uniform(512, 256)\n",
    "        self.l4 = Tensor.scaled_uniform(256, 2)\n",
    "    \n",
    "    def __call__(self, x: Tensor) -> Tensor:\n",
    "        x = x.dot(self.l1).leakyrelu(0.2).dropout(0.3)\n",
    "        x = x.dot(self.l2).leakyrelu(0.2).dropout(0.3)\n",
    "        x = x.dot(self.l3).leakyrelu(0.2).dropout(0.3)\n",
    "        x = x.dot(self.l4).log_softmax()\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_batch(images: np.ndarray) -> Tensor:\n",
    "    sample = np.random.randint(0, high=len(images), size=(batch_size))\n",
    "    image_b = images[sample].reshape(-1, 28 * 28).astype(np.float32) / 255.0\n",
    "    image_b = (image_b - 0.5) / 0.5\n",
    "    return Tensor(image_b)\n",
    "\n",
    "\n",
    "def make_labels(bs: int, value: int) -> Tensor:\n",
    "    y = np.zeros((bs, 2), dtype=np.float32)\n",
    "    y[range(bs), [value] * bs] = -2.0\n",
    "    return Tensor(y)\n",
    "\n",
    "\n",
    "def train_discriminator(optimizer: optim.Optimizer, real_data: Tensor, fake_data: Tensor) -> np.ndarray:\n",
    "    real_labels = make_labels(batch_size, 1)\n",
    "    fake_labels = make_labels(batch_size, 0)\n",
    "    optimizer.zero_grad()\n",
    "    real_output = discriminator(real_data)\n",
    "    fake_output = discriminator(fake_data)\n",
    "    real_loss = (real_output * real_labels).mean()\n",
    "    fake_loss = (fake_output * fake_labels).mean()\n",
    "    real_loss.backward()\n",
    "    fake_loss.backward()\n",
    "    optimizer.step()\n",
    "    return (real_loss + fake_loss).cpu().numpy()\n",
    "\n",
    "\n",
    "def train_generator(optimizer: optim.Optimizer, fake_data: Tensor) -> np.ndarray:\n",
    "    real_labels = make_labels(batch_size, 1)\n",
    "    optimizer.zero_grad()\n",
    "    output = discriminator(fake_data)\n",
    "    loss = (output * real_labels).mean()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "epochs = 300\n",
    "sample_interval = 50\n",
    "\n",
    "real_images = np.vstack(fetch_mnist()[::2])\n",
    "n_steps = len(real_images) // batch_size\n",
    "ds_noise = Tensor(np.random.randn(20, 100), requires_grad=False)\n",
    "\n",
    "generator = Generator(100, 784)\n",
    "discriminator = Discriminator(784)\n",
    "g_optim = optim.Adam(optim.get_parameters(generator), lr=0.0002, b1=0.5)\n",
    "d_optim = optim.Adam(optim.get_parameters(discriminator), lr=0.0002, b1=0.5)\n",
    "\n",
    "for epoch in (t := trange(epochs)):\n",
    "    g_loss, d_loss = 0.0, 0.0\n",
    "    for _ in range(n_steps):\n",
    "        real_data = make_batch(real_images)\n",
    "        noise = Tensor(np.random.randn(batch_size, 100))\n",
    "        d_loss += train_discriminator(d_optim, real_data, generator(noise).detach())\n",
    "        g_loss += train_generator(g_optim, generator(noise))\n",
    "    if (epoch + 1) % sample_interval == 0:\n",
    "        plt.figure(figsize=(16, 16))\n",
    "        output = generator(ds_noise).detach().cpu().numpy()\n",
    "        plt.imshow(np.hstack(output.reshape(-1, 28, 28)), cmap=\"gray\")\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n",
    "    t.set_description(f\"g_loss {g_loss/n_steps} d_loss {d_loss/n_steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7e1998ff7f8aa20ada591c520b972326324e5ea05489af9e422744c7c09f6dad"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
