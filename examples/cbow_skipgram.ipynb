{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf163ff7-1bbc-41ee-bacd-f31e784eea4a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x104d926d0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from functools import partial\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchtext.vocab import build_vocab_from_iterator \n",
    "from torchtext.data import to_map_style_dataset, utils\n",
    "from torchtext.datasets import WikiText2\n",
    "torch.manual_seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "247bd357-3bfb-4438-92aa-d955d027d2c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "embedding_dim = 300\n",
    "max_norm = 1\n",
    "min_freq = 50\n",
    "num_words = 4\n",
    "max_sequence_length = 256\n",
    "\n",
    "data_dir = \"../data\"\n",
    "epochs = 10\n",
    "batch_size = 96\n",
    "learning_rate = 0.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f728ae6d-cd10-460c-9cd8-2dbb6f09e4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOW(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim=embedding_dim, max_norm=max_norm)\n",
    "        self.fc = nn.Linear(embedding_dim, vocab_size)\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        x = self.embeddings(x)\n",
    "        x = x.mean(axis=1)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f87b4e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(data_iter, tokenizer): \n",
    "    vocab = build_vocab_from_iterator(map(tokenizer, data_iter), specials=[\"<unk>\"], min_freq=min_freq)\n",
    "    vocab.set_default_index(vocab[\"<unk>\"])\n",
    "    return vocab\n",
    "\n",
    "\n",
    "def collate_cbow(batch, text_pipeline):\n",
    "    inputs, outputs = [], []\n",
    "    for text in batch:\n",
    "        token_ids = text_pipeline(text)\n",
    "        if len(token_ids) < num_words * 2 + 1:\n",
    "            continue\n",
    "        token_ids = token_ids[:max_sequence_length]\n",
    "        for i in range(len(token_ids) - num_words * 2):\n",
    "            sequence = token_ids[i:num_words*2+i+1]\n",
    "            outputs.append(sequence.pop(num_words))\n",
    "            inputs.append(sequence)\n",
    "        return torch.tensor(inputs, dtype=torch.long), torch.tensor(outputs, dtype=torch.long)\n",
    "\n",
    "\n",
    "def get_dataloader_and_vocab(data_dir, ds_type, batch_size, shuffle=True, vocab=None):\n",
    "    data_iter = to_map_style_dataset(WikiText2(root=data_dir, split=ds_type))\n",
    "    tokenizer = utils.get_tokenizer(\"basic_english\", language=\"en\")\n",
    "    if not vocab:\n",
    "        vocab = build_vocab(data_iter, tokenizer)\n",
    "    collate_fn = partial(collate_cbow, text_pipeline=lambda x: vocab(tokenizer(x)))\n",
    "    dataloader = DataLoader(data_iter, batch_size=batch_size, shuffle=shuffle, collate_fn=collate_fn)\n",
    "    return dataloader, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25e10701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 4099\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train loss 10.09: 100%|█████████████████████████████████████████████████| 383/383 [00:03<00:00, 112.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss 3.68\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train loss 6.14: 100%|██████████████████████████████████████████████████| 383/383 [00:03<00:00, 114.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss 3.97\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train loss 6.67: 100%|██████████████████████████████████████████████████| 383/383 [00:03<00:00, 111.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss 3.99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train loss 5.46: 100%|██████████████████████████████████████████████████| 383/383 [00:03<00:00, 111.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss 4.04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train loss 5.58: 100%|██████████████████████████████████████████████████| 383/383 [00:03<00:00, 109.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss 3.83\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train loss 5.21: 100%|██████████████████████████████████████████████████| 383/383 [00:03<00:00, 111.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss 4.05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train loss 4.98: 100%|██████████████████████████████████████████████████| 383/383 [00:03<00:00, 110.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss 3.97\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train loss 4.61: 100%|██████████████████████████████████████████████████| 383/383 [00:03<00:00, 108.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss 3.81\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train loss 5.45: 100%|██████████████████████████████████████████████████| 383/383 [00:03<00:00, 110.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss 3.90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train loss 5.55: 100%|██████████████████████████████████████████████████| 383/383 [00:03<00:00, 110.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss 3.68\n"
     ]
    }
   ],
   "source": [
    "train_dataloader, vocab = get_dataloader_and_vocab(data_dir, \"train\", batch_size)\n",
    "valid_dataloader, _ = get_dataloader_and_vocab(data_dir, \"valid\", batch_size)\n",
    "vocab_size = len(vocab.get_stoi())\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "train_steps, valid_steps = len(train_dataloader), len(valid_dataloader)\n",
    "\n",
    "model = CBOW(vocab_size)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for _ in range(epochs):\n",
    "    model.train()\n",
    "    for i, (x, y) in (t := tqdm(enumerate(train_dataloader), total=train_steps)):\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(model(x), y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        t.set_description(f\"train loss {loss.item():.2f}\")\n",
    "    model.eval()\n",
    "    total = 0.0\n",
    "    for i, (x, y) in enumerate(valid_dataloader):\n",
    "        loss = criterion(model(x), y)\n",
    "        total += loss.item()\n",
    "    print(f\"validation loss {(total/valid_steps):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57d02548-4dcc-4e5d-9500-ac0347ac1de8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4099, 300)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = next(model.parameters()).detach().numpy()\n",
    "norms = ((embeddings ** 2).sum(axis=1) ** 0.5).reshape(-1, 1)\n",
    "embeddings_norm = embeddings / norms\n",
    "embeddings_norm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4438cfd5-f9f5-4ef5-bb82-86f7dc00e96e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writings: 0.849\n",
      "honour: 0.825\n",
      "genre: 0.812\n",
      "religion: 0.812\n",
      "mother: 0.806\n",
      "doubt: 0.802\n",
      "speaking: 0.799\n",
      "faced: 0.794\n",
      "calvert: 0.789\n",
      "friend: 0.785\n"
     ]
    }
   ],
   "source": [
    "def get_similar(word, n=10):\n",
    "    word_id = vocab[word]\n",
    "    if word_id == 0:\n",
    "        print(\"out of vocabulary word\")\n",
    "        return {}\n",
    "    word_vec = embeddings_norm[word_id].flatten()\n",
    "    dists = np.matmul(embeddings_norm, word_vec).flatten()\n",
    "    top_ids = np.argsort(-dists)[1:n+1]\n",
    "    top_dict = {}\n",
    "    for sim_word_id in top_ids:\n",
    "        sim_word = vocab.lookup_token(sim_word_id)\n",
    "        top_dict[sim_word] = dists[sim_word_id]\n",
    "    return top_dict\n",
    "\n",
    "\n",
    "for word, score in get_similar(\"father\").items():\n",
    "    print(f\"{word}: {score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9bfae95c-1409-4c85-ad36-357657fa44e2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "king: 0.867\n",
      "queen: 0.832\n",
      "assistant: 0.824\n",
      "harbour: 0.819\n",
      "attributed: 0.809\n",
      "howard: 0.808\n",
      "haiti: 0.808\n",
      "legend: 0.804\n",
      "mode: 0.803\n",
      "lennon: 0.795\n"
     ]
    }
   ],
   "source": [
    "emb = embeddings[vocab[\"king\"]] - embeddings[vocab[\"man\"]] + embeddings[vocab[\"woman\"]]\n",
    "norm = (emb ** 2).sum() ** 0.5\n",
    "emb_norm = (emb / norm).flatten()\n",
    "dists = np.matmul(embeddings_norm, emb_norm).flatten()\n",
    "word_ids = np.argsort(-dists)[:10]\n",
    "\n",
    "for word_id in word_ids:\n",
    "    print(f\"{vocab.lookup_token(word_id)}: {dists[word_id]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "64789d8b-0919-4898-bea7-f5da4d2caa14",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGram(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim=embedding_dim, max_norm=max_norm)\n",
    "        self.fc = nn.Linear(embedding_dim, vocab_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embeddings(x)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6520c1ee-ec5c-4249-84f7-695049fc9229",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_skipgram(batch, text_pipeline):\n",
    "    inputs, outputs = [], []\n",
    "    for text in batch:\n",
    "        token_ids = text_pipeline(text)\n",
    "        if len(token_ids) < num_words * 2 + 1:\n",
    "            continue\n",
    "        token_ids = token_ids[:max_sequence_length]\n",
    "        for i in range(len(token_ids) - num_words * 2):\n",
    "            sequence = token_ids[i:num_words*2+i+1]\n",
    "            input_ = sequence.pop(num_words)\n",
    "            for output in sequence:\n",
    "                inputs.append(input_)\n",
    "                outputs.append(output)\n",
    "    return torch.tensor(inputs, dtype=torch.long), torch.tensor(outputs, dtype=torch.long)\n",
    "    \n",
    "    \n",
    "def get_dataloader_and_vocab(data_dir, ds_type, batch_size, shuffle=True, vocab=None):\n",
    "    data_iter = to_map_style_dataset(WikiText2(root=data_dir, split=ds_type))\n",
    "    tokenizer = utils.get_tokenizer(\"basic_english\", language=\"en\")\n",
    "    if not vocab:\n",
    "        vocab = build_vocab(data_iter, tokenizer)\n",
    "    collate_fn = partial(collate_skipgram, text_pipeline=lambda x: vocab(tokenizer(x)))\n",
    "    dataloader = DataLoader(data_iter, batch_size=batch_size, shuffle=shuffle, collate_fn=collate_fn)\n",
    "    return dataloader, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d5d0d8a1-5bbf-4084-8efb-b866b50faeeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 4099\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train loss 5.57: 100%|███████████████████████████████████████████████████| 383/383 [03:14<00:00,  1.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss 3.96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train loss 5.48: 100%|███████████████████████████████████████████████████| 383/383 [03:14<00:00,  1.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss 4.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train loss 5.40: 100%|███████████████████████████████████████████████████| 383/383 [03:14<00:00,  1.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss 3.98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train loss 5.47: 100%|███████████████████████████████████████████████████| 383/383 [03:14<00:00,  1.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss 3.97\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train loss 5.50: 100%|███████████████████████████████████████████████████| 383/383 [03:14<00:00,  1.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss 3.96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train loss 5.47: 100%|███████████████████████████████████████████████████| 383/383 [03:14<00:00,  1.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss 3.96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train loss 5.35: 100%|███████████████████████████████████████████████████| 383/383 [03:14<00:00,  1.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss 3.98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train loss 5.40: 100%|███████████████████████████████████████████████████| 383/383 [03:14<00:00,  1.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss 3.95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train loss 5.42: 100%|███████████████████████████████████████████████████| 383/383 [03:14<00:00,  1.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss 3.95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train loss 5.45: 100%|███████████████████████████████████████████████████| 383/383 [03:15<00:00,  1.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss 3.98\n"
     ]
    }
   ],
   "source": [
    "train_dataloader, vocab = get_dataloader_and_vocab(data_dir, \"train\", batch_size)\n",
    "valid_dataloader, _ = get_dataloader_and_vocab(data_dir, \"valid\", batch_size)\n",
    "vocab_size = len(vocab.get_stoi())\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "train_steps, valid_steps = len(train_dataloader), len(valid_dataloader)\n",
    "\n",
    "model = SkipGram(vocab_size)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for _ in range(epochs):\n",
    "    model.train()\n",
    "    for i, (x, y) in (t := tqdm(enumerate(train_dataloader), total=train_steps)):\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(model(x), y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        t.set_description(f\"train loss {loss.item():.2f}\")\n",
    "    model.eval()\n",
    "    total = 0.0\n",
    "    for i, (x, y) in enumerate(valid_dataloader):\n",
    "        loss = criterion(model(x), y)\n",
    "        total += loss.item()\n",
    "    print(f\"validation loss {(total/valid_steps):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a4fb648d-efb2-4270-88e8-3a44d9948a82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4099, 300)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = next(model.parameters()).detach().numpy()\n",
    "norms = ((embeddings ** 2).sum(axis=1) ** 0.5).reshape(-1, 1)\n",
    "embeddings_norm = embeddings / norms\n",
    "embeddings_norm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "77648e70-1055-4dee-be5f-93c168c1522b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mother: 0.797\n",
      "wife: 0.765\n",
      "brother: 0.763\n",
      "son: 0.741\n",
      "husband: 0.717\n",
      "sister: 0.708\n",
      "pitman: 0.704\n",
      "isabella: 0.677\n",
      "daughter: 0.676\n",
      "rosebery: 0.674\n"
     ]
    }
   ],
   "source": [
    "for word, score in get_similar(\"father\").items():\n",
    "    print(f\"{word}: {score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e94d5394-6f73-42eb-99a8-0f5d6a5ea1f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "king: 0.702\n",
      "woman: 0.629\n",
      "son: 0.547\n",
      "banai: 0.516\n",
      "isabella: 0.513\n",
      "goddess: 0.513\n",
      "philip: 0.508\n",
      "edward: 0.507\n",
      "queen: 0.506\n",
      "jesus: 0.504\n"
     ]
    }
   ],
   "source": [
    "emb = embeddings[vocab[\"king\"]] - embeddings[vocab[\"man\"]] + embeddings[vocab[\"woman\"]]\n",
    "norm = (emb ** 2).sum() ** 0.5\n",
    "emb_norm = (emb / norm).flatten()\n",
    "dists = np.matmul(embeddings_norm, emb_norm).flatten()\n",
    "word_ids = np.argsort(-dists)[:10]\n",
    "\n",
    "for word_id in word_ids:\n",
    "    print(f\"{vocab.lookup_token(word_id)}: {dists[word_id]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83953a8-4017-4f23-b38f-25c87d0e7c49",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
